{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ceeeb46",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "Jiss Xavier, 916427256\n",
    "\n",
    "For this assignment, you will be developing an artificial neural network to classify data given in the __[Dry Beans Data Set](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset#)__. This data set was obtained as a part of a research study by Selcuk University, Turkey, in which a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features. More details on the study can be found in the following __[research paper](https://www.sciencedirect.com/science/article/pii/S0168169919311573)__.\n",
    "\n",
    "## About the Data Set\n",
    "Seven different types of dry beans were used in a study in Selcuk University, Turkey, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features - 12 dimensions and 4 shape forms - were obtained from the grains.\n",
    "\n",
    "Number of Instances (records in the data set): __13611__\n",
    "\n",
    "Number of Attributes (fields within each record, including the class): __17__\n",
    "\n",
    "### Data Set Attribute Information:\n",
    "\n",
    "1. __Area (A)__ : The area of a bean zone and the number of pixels within its boundaries.\n",
    "2. __Perimeter (P)__ : Bean circumference is defined as the length of its border.\n",
    "3. __Major axis length (L)__ : The distance between the ends of the longest line that can be drawn from a bean.\n",
    "4. __Minor axis length (l)__ : The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
    "5. __Aspect ratio (K)__ : Defines the relationship between L and l.\n",
    "6. __Eccentricity (Ec)__ : Eccentricity of the ellipse having the same moments as the region.\n",
    "7. __Convex area (C)__ : Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
    "8. __Equivalent diameter (Ed)__ : The diameter of a circle having the same area as a bean seed area.\n",
    "9. __Extent (Ex)__ : The ratio of the pixels in the bounding box to the bean area.\n",
    "10. __Solidity (S)__ : Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11. __Roundness (R)__ : Calculated with the following formula: (4piA)/(P^2)\n",
    "12. __Compactness (CO)__ : Measures the roundness of an object: Ed/L\n",
    "13. __ShapeFactor1 (SF1)__\n",
    "14. __ShapeFactor2 (SF2)__\n",
    "15. __ShapeFactor3 (SF3)__\n",
    "16. __ShapeFactor4 (SF4)__\n",
    "\n",
    "17. __Classes : *Seker, Barbunya, Bombay, Cali, Dermosan, Horoz, Sira*__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61beac74",
   "metadata": {},
   "source": [
    "### Libraries that can be used :\n",
    "- NumPy, SciPy, Pandas, Sci-Kit Learn, TensorFlow, Keras\n",
    "- Any other library used during the lectures and discussion sessions.\n",
    "\n",
    "### Other Notes\n",
    "- Don't worry about not being able to achieve high accuracy, it is neither the goal nor the grading standard of this assignment.\n",
    "- Discussion materials should be helpful for doing the assignments.\n",
    "- The homework submission should be a .ipynb file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f58b7",
   "metadata": {},
   "source": [
    "Worked on high level concepts with Tejes Srivastava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264c751",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 : Building a Feed-Forward Neural Network(50 points)\n",
    "\n",
    "### Exercise 1.1 : Data Preprocessing (10 points)\n",
    "\n",
    "- As the classes are categorical, use one-hot encoding to represent the set of classes. You will find this useful when developing the output layer of the neural network.\n",
    "- Normalize each field of the input data using the min-max normalization technique.\n",
    "\n",
    "### Exercise 1.2 : Training and Testing the Neural Network (40 points)\n",
    "\n",
    "Design a 4-layer artificial neural network, specifically a feed-forward multi-layer perceptron (using the sigmoid activation function), to classify the type of 'Dry Bean' given the other attributes in the data set, similar to the one mentioned in the paper above. Please note that this is a multi-class classification problem so select the right number of nodes accordingly for the output layer.\n",
    "\n",
    "For training and testing the model, split the data into training and testing set by __90:10__ and use the training set for training the model and the test set to evaluate the model performance.\n",
    "\n",
    "Consider the following hyperparameters while developing your model :\n",
    "\n",
    "- Number of nodes in each hidden layer should be (12, 3)\n",
    "- Learning rate should be 0.3\n",
    "- Number of epochs should be 500\n",
    "- The sigmoid function should be used as the activation function in each layer\n",
    "- Stochastic Gradient Descent should be used to minimize the error rate\n",
    "\n",
    "__Requirements once the model has been trained :__\n",
    "\n",
    "- A confusion matrix for all classes, specifying the true positive, true negative, false positive, and false negative cases for each category in the class\n",
    "- The accuracy and mean squared error (MSE) of the model\n",
    "- The precision and recall for each label in the class\n",
    "\n",
    "__Notes :__\n",
    "\n",
    "- Splitting of the dataset should be done __after__ the data preprocessing step.\n",
    "- The mean squared error (MSE) values obtained __should be positive__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85173878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 3 3 ... 6 4 6]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Based off One Hot Encoding example from 10/08 Discussion\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Read in the file\n",
    "beans_df = pd.read_csv('./Dry_Beans_Dataset.csv')\n",
    "\n",
    "#Label Encoding\n",
    "myData_encoder = LabelEncoder()\n",
    "myData_encoded = myData_encoder.fit_transform(beans_df['Class'])\n",
    "print(myData_encoded)\n",
    "\n",
    "\n",
    "#One Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# reshape the array\n",
    "myData_encoded = myData_encoded.reshape(len(myData_encoded), 1) \n",
    "onehot_encoded = onehot_encoder.fit_transform(myData_encoded)\n",
    "\n",
    "print(onehot_encoded)\n",
    "\n",
    "#Normalizing each field of the input data using the min-max normalization technique\n",
    "sc_X = MinMaxScaler()\n",
    "X_scaled = sc_X.fit_transform(beans_df.drop(columns=['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c87f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of training model: 0.02512207254767418\n",
      "MSE of testing model: 0.025625422596931458\n",
      "Accuracy of training model: 0.8766430020332336\n",
      "Accuracy of testing model: 0.8744493126869202\n",
      "[[1166   50]\n",
      " [  15  131]]\n",
      "Recall: 0.9588815789473685\n",
      "Precision: 0.9872988992379339\n",
      "[[1304    0]\n",
      " [  58    0]]\n",
      "Recall: 1.0\n",
      "Precision: 0.9574155653450808\n",
      "[[1158   36]\n",
      " [  12  156]]\n",
      "Recall: 0.9698492462311558\n",
      "Precision: 0.9897435897435898\n",
      "[[976  36]\n",
      " [ 25 325]]\n",
      "Recall: 0.9644268774703557\n",
      "Precision: 0.975024975024975\n",
      "[[1155    6]\n",
      " [   9  192]]\n",
      "Recall: 0.9948320413436692\n",
      "Precision: 0.9922680412371134\n",
      "[[1179    8]\n",
      " [  11  164]]\n",
      "Recall: 0.9932603201347936\n",
      "Precision: 0.9907563025210084\n",
      "[[1063   35]\n",
      " [  41  223]]\n",
      "Recall: 0.9681238615664846\n",
      "Precision: 0.9628623188405797\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "#Split the data to training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, onehot_encoded, test_size=0.1)\n",
    "\n",
    "model = Sequential()\n",
    "#Number of nodes in the first hidden layer\n",
    "model.add(Dense(12, input_dim=16, activation='sigmoid'))\n",
    "#Number of nodes in the second hidden layer\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "#Maps to the 7 different types of bean\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "#From Discussion 10/15\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=SGD(learning_rate=0.3),\n",
    "              metrics=['accuracy', 'mse'])\n",
    "\n",
    "#From Discussion 10/15\n",
    "model.fit(X_train,\n",
    "          Y_train, \n",
    "          epochs=500, \n",
    "          verbose=0)\n",
    "\n",
    "#Find training/testing accuracy and MSE\n",
    "_, training_accuracy, training_mse = model.evaluate(X_train, Y_train, verbose=0)\n",
    "_, testing_accuracy, testing_mse = model.evaluate(X_test, Y_test, verbose=0)\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "#Print the MSE and Accuracy of the models\n",
    "print('MSE of training model:', training_mse)\n",
    "print('MSE of testing model:', testing_mse)\n",
    "print('Accuracy of training model:', training_accuracy)\n",
    "print('Accuracy of testing model:', testing_accuracy)\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "for i in multilabel_confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1)):\n",
    "    print(i)\n",
    "    print('Recall:', (i[0][0])/(i[0][0] + i[0][1]))\n",
    "    print('Precision:',(i[0][0])/(i[0][0] + i[1][0]))\n",
    "    \n",
    "#Formula for recall and precision: https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f5188",
   "metadata": {},
   "source": [
    "## Exercise 2 : k-fold Cross Validation (20 points)\n",
    "\n",
    "In order to avoid using biased models, use 10-fold cross validation to generalize the model based on the given data set.\n",
    "\n",
    "__Requirements :__\n",
    "- The accuracy and MSE values during each iteration of the cross validation\n",
    "- The overall average accuracy and MSE value\n",
    "\n",
    "__Note :__ The mean squared error (MSE) values obtained should be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dd4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Iteration # 1 :\n",
      "\t Accuracy: 0.8869310021400452\n",
      "\t MSE: 0.02343909442424774\n",
      "Cross Validation Iteration # 2 :\n",
      "\t Accuracy: 0.886113166809082\n",
      "\t MSE: 0.02304946258664131\n",
      "Cross Validation Iteration # 3 :\n",
      "\t Accuracy: 0.9118295311927795\n",
      "\t MSE: 0.02214258909225464\n",
      "Cross Validation Iteration # 4 :\n",
      "\t Accuracy: 0.9155033230781555\n",
      "\t MSE: 0.02224731259047985\n",
      "Cross Validation Iteration # 5 :\n",
      "\t Accuracy: 0.9316678643226624\n",
      "\t MSE: 0.020588815212249756\n",
      "Cross Validation Iteration # 6 :\n",
      "\t Accuracy: 0.920646607875824\n",
      "\t MSE: 0.02184421569108963\n",
      "Cross Validation Iteration # 7 :\n",
      "\t Accuracy: 0.9294636249542236\n",
      "\t MSE: 0.019694708287715912\n",
      "Cross Validation Iteration # 8 :\n",
      "\t Accuracy: 0.9338721632957458\n",
      "\t MSE: 0.01894899643957615\n",
      "Cross Validation Iteration # 9 :\n",
      "\t Accuracy: 0.9140337705612183\n",
      "\t MSE: 0.02210644632577896\n",
      "Cross Validation Iteration # 10 :\n",
      "\t Accuracy: 0.9191770553588867\n",
      "\t MSE: 0.02302875928580761\n",
      "Overall Average Accuracy : 0.9149238109588623\n",
      "Overall Average MSE: 0.021709039993584155\n"
     ]
    }
   ],
   "source": [
    "#Takes around 20 min to compile and execute\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#initalize values\n",
    "overall_accuracy = 0\n",
    "overall_mse = 0\n",
    "iterator = 0\n",
    "\n",
    "#Create the model\n",
    "\n",
    "model = Sequential()\n",
    "#Number of nodes in the first hidden layer\n",
    "model.add(Dense(12, input_dim=16, activation='sigmoid'))\n",
    "#Number of nodes in the second hidden layer\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "#Maps to the 7 different types of bean\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "#From Discussion 10/15\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=SGD(learning_rate=0.3),\n",
    "              metrics=['accuracy', 'mse'])\n",
    "\n",
    "#10 fold cross validation\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "#Loop through each iteration of the cross validation\n",
    "for trainingIndex, testingIndex in kf.split(X_scaled, onehot_encoded):\n",
    "    X_train, X_test = X_scaled[trainingIndex], X_scaled[testingIndex]\n",
    "    y_train, y_test = onehot_encoded[trainingIndex], onehot_encoded[testingIndex]\n",
    "    \n",
    "    #From Discussion 10/15\n",
    "    model.fit(X_train, \n",
    "              y_train, \n",
    "              epochs=500, \n",
    "              verbose=0)\n",
    "    \n",
    "    #Find testing accuracy and MSE\n",
    "    _, testing_accuracy, testing_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Cross Validation Iteration #', iterator+1, ':')\n",
    "    print('\\t','Accuracy:', testing_accuracy)\n",
    "    print('\\t','MSE:', testing_mse)\n",
    "    iterator+=1\n",
    "    overall_accuracy += testing_accuracy\n",
    "    overall_mse += testing_mse\n",
    "\n",
    "print('Overall Average Accuracy :', overall_accuracy/10)\n",
    "print('Overall Average MSE:', overall_mse/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef874c5",
   "metadata": {},
   "source": [
    "## Exercise 3 : Hyperparameter Tuning (30 points)\n",
    "\n",
    "Use either grid search or random search methodology to find the optimal number of nodes required in each hidden layer, as well as the optimal learning rate and the number of epochs, such that the accuracy of the model is maximum for the given data set.\n",
    "\n",
    "__Requirements :__\n",
    "- The set of optimal hyperparameters\n",
    "- The maximum accuracy achieved using this set of optimal hyperparameters\n",
    "\n",
    "__Note :__ Hyperparameter tuning takes a lot of time to execute. Make sure that you choose the appropriate number of each hyperparameter (preferably 3 of each), and that you allocate enough time to execute your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5572c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe602f20a60>\n",
      "Mean test score of 0.101756 and standard deviation of (0.015757) are obtained with: {'learnRate': 0.5, 'layer2': 3, 'layer1': 15, 'epochs': 900}\n",
      "Mean test score of 0.126809 and standard deviation of (0.091969) are obtained with: {'learnRate': 0.1, 'layer2': 3, 'layer1': 10, 'epochs': 300}\n",
      "Mean test score of 0.130409 and standard deviation of (0.002926) are obtained with: {'learnRate': 0.25, 'layer2': 6, 'layer1': 10, 'epochs': 900}\n",
      "Mean test score of 0.171112 and standard deviation of (0.022588) are obtained with: {'learnRate': 0.1, 'layer2': 6, 'layer1': 5, 'epochs': 300}\n",
      "Mean test score of 0.111234 and standard deviation of (0.021690) are obtained with: {'learnRate': 0.1, 'layer2': 3, 'layer1': 15, 'epochs': 300}\n",
      "Mean test score of 0.137389 and standard deviation of (0.016635) are obtained with: {'learnRate': 0.25, 'layer2': 3, 'layer1': 5, 'epochs': 300}\n",
      "Mean test score of 0.090074 and standard deviation of (0.003510) are obtained with: {'learnRate': 0.5, 'layer2': 17, 'layer1': 10, 'epochs': 600}\n",
      "Mean test score of 0.114834 and standard deviation of (0.013405) are obtained with: {'learnRate': 0.5, 'layer2': 3, 'layer1': 10, 'epochs': 900}\n",
      "Mean test score of 0.061788 and standard deviation of (0.021599) are obtained with: {'learnRate': 0.5, 'layer2': 17, 'layer1': 15, 'epochs': 300}\n",
      "Mean test score of 0.120638 and standard deviation of (0.010918) are obtained with: {'learnRate': 0.1, 'layer2': 6, 'layer1': 15, 'epochs': 600}\n",
      "\n",
      " The best score observed is 0.171112 and it is obtained using {'learnRate': 0.1, 'layer2': 6, 'layer1': 5, 'epochs': 300}\n"
     ]
    }
   ],
   "source": [
    "#Takes a long time to execute, might have to run again to view results on your local machine.\n",
    "#Code is working, may just have to run it again to view results.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#Based of code RandomizedSearch from Discussion 10/20\n",
    "\n",
    "#function from discussion to create the model\n",
    "def create_model(layer1=12, layer2=3, learnRate=0.3):\n",
    "    model = Sequential()\n",
    "    #Number of nodes in the first hidden layer\n",
    "    model.add(Dense(layer1, input_dim=16, activation='sigmoid'))\n",
    "    #Number of nodes in the second hidden layer\n",
    "    model.add(Dense(layer2, activation='sigmoid'))\n",
    "    #Maps to the 7 different types of bean\n",
    "    model.add(Dense(7, activation='sigmoid'))\n",
    "    #From Discussion 10/15\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=SGD(learning_rate=learnRate),\n",
    "                  metrics=['accuracy', 'mse'])\n",
    "    return model\n",
    "\n",
    "#Use the function to create the model for randomSearch\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "print(model)\n",
    "\n",
    "#sample model hyperparameters\n",
    "layer1=[5, 10, 15]\n",
    "layer2=[3, 6, 17]\n",
    "learnRate=[0.1, .25, 0.5]\n",
    "epochs=[300, 600, 900]\n",
    "\n",
    "#Creating dictionary to be used for randomizedSearchCV\n",
    "randomSearchDict = dict(layer1 = layer1,\n",
    "                    layer2 = layer2,\n",
    "                    epochs=epochs, \n",
    "                    learnRate = learnRate) \n",
    "\n",
    "#run RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(estimator=model, \n",
    "                          param_distributions=randomSearchDict, \n",
    "                          cv=3)\n",
    "#get the results\n",
    "randomSearch = rs.fit(X_scaled, onehot_encoded.argmax(axis=1))\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/64209804/hyperparameter-tuning-with-gridsearch-with-various-parameters\n",
    "meanScore = randomSearch.cv_results_['mean_test_score']\n",
    "stdTestScore = randomSearch.cv_results_['std_test_score']\n",
    "params = randomSearch.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(meanScore, stdTestScore, params):\n",
    "    print(\"Mean test score of %f and standard deviation of (%f) are obtained with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(\"\\n\",\"The best score observed is %f and it is obtained using %s\" % (randomSearch.best_score_, randomSearch.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedfdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
