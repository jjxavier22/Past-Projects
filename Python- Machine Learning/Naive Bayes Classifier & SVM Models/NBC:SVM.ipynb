{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "Jiss Xavier, 916427256\n",
    "\n",
    "In this assignment, we will be building a Naïve Bayes classifier and a SVM model for the productivity satisfaction of [the given dataset](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees), the productivity of garment employees.\n",
    "\n",
    "## Background\n",
    "The Garment Industry is one of the key examples of the industrial globalization of this modern era. It is a highly labour-intensive industry with lots of manual processes. Satisfying the huge global demand for garment products is mostly dependent on the production and delivery performance of the employees in the garment manufacturing companies. So, it is highly desirable among the decision makers in the garments industry to track, analyse and predict the productivity performance of the working teams in their factories. \n",
    "\n",
    "## Dataset Attribute Information\n",
    "\n",
    "1. **date**: Date in MM-DD-YYYY\n",
    "2. **day**: Day of the Week\n",
    "3. **quarter** : A portion of the month. A month was divided into four quarters\n",
    "4. **department** : Associated department with the instance\n",
    "5. **team_no** : Associated team number with the instance\n",
    "6. **no_of_workers** : Number of workers in each team\n",
    "7. **no_of_style_change** : Number of changes in the style of a particular product\n",
    "8. **targeted_productivity** : Targeted productivity set by the Authority for each team for each day.\n",
    "9. **smv** : Standard Minute Value, it is the allocated time for a task\n",
    "10. **wip** : Work in progress. Includes the number of unfinished items for products\n",
    "11. **over_time** : Represents the amount of overtime by each team in minutes\n",
    "12. **incentive** : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n",
    "13. **idle_time** : The amount of time when the production was interrupted due to several reasons\n",
    "14. **idle_men** : The number of workers who were idle due to production interruption\n",
    "15. **actual_productivity** : The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n",
    "\n",
    "### Libraries that can be used: numpy, scipy, pandas, scikit-learn, cvxpy, imbalanced-learn\n",
    "Any libraries used in the discussion materials are also allowed.\n",
    "\n",
    "#### Other Notes\n",
    "\n",
    " - Don't worry about not being able to achieve high accuracy, it is neither the goal nor the grading standard of this assignment. <br >\n",
    " - If not specified, you are not required to do hyperparameter tuning, but feel free to do so if you'd like.\n",
    "\n",
    "#### Trouble Shooting\n",
    "In case you have trouble installing and using imbalanced-learn(imblearn) <br >\n",
    "Run the below code cell, then go to the selection bar at top: Kernel > Restart. <br >\n",
    "Then try `import imblearn` to see if things work. \n",
    "\n",
    "\n",
    "Worked on high level concepts with Tejes Srivastava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Applications/anaconda3/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: delayed in /Applications/anaconda3/lib/python3.8/site-packages (0.11.0b1)\n",
      "Requirement already satisfied: hiredis in /Applications/anaconda3/lib/python3.8/site-packages (from delayed) (2.0.0)\n",
      "Requirement already satisfied: redis in /Applications/anaconda3/lib/python3.8/site-packages (from delayed) (3.5.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Applications/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /Applications/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Applications/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /Applications/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install imbalanced-learn delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 1 - General Data Preprocessing (20 points)\n",
    "\n",
    "Our dataset needs cleaning before building any models. Some of the cleaning tasks are common in general, but depends on what kind of models we are building, sometimes we have to do additional processing. These additional tasks will be mentioned in each of the remaining two exercises later.\n",
    "\n",
    "Note that **we will be using this processed data from exercise 1 in each of the remaining two exercises**.\n",
    "\n",
    "For convenience, here are the attributes that we would treat them as **categorical attributes**: `day`, `quarter`, `department`, and `team`. \n",
    "\n",
    " - Drop the column `date`.\n",
    " - For each of the categorical attributes, **print out** all the unique elements.\n",
    " - For each of the categorical attributes, remap the duplicated items, if you find there are typos or spaces among the duplicated items.\n",
    "     - For example, \"a\" and \"a \" should be the same, so we need to update \"a \" to be \"a\".\n",
    "     - Another example, \"apple\" and \"appel\" should be the same, so you should update \"appel\" to be \"apple\".\n",
    "     \n",
    "\n",
    " - Create another column named `satisfied` that records the productivity performance. The behavior defined as follows. **This is the dependent variable we'd like to classify in this assignment.**\n",
    "     - Return True or 1 if `actual_productivity` is equal to or greater than `targeted_productivity`. Otherwise, return False or 0, which means the team fails to meet the expected performance.\n",
    " - Drop the columns `actual_productivity` and `targeted_productivity`.\n",
    "\n",
    "\n",
    " - Find and **print out** which columns/attributes that have empty vaules, e.g., NA, NaN, null, None.\n",
    " - Fill the empty values with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique days:  ['Thursday' 'Saturday' 'Sunday' 'Monday' 'Tuesday' 'Wednesday'] \n",
      "Unique quarter attributes:  ['Quarter1' 'Quarter2' 'Quarter3' 'Quarter4' 'Quarter5'] \n",
      "Unique department attributes:  ['sweing' 'finishing ' 'finishing'] \n",
      "Unique team attributes:  [ 8  1 11 12  6  7  2  3  9 10  5  4]\n",
      "\n",
      "Unique department attributes (after fixing typos): ['sweing' 'finishing']\n",
      "\n",
      "Columns/Attributes with empty values: ['wip']\n"
     ]
    }
   ],
   "source": [
    "# If put the data(.csv) under the same folder, you could use\n",
    "# df = pd.read_csv('./garments_worker_productivity.csv')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#reads in the file using the command provided above\n",
    "df = pd.read_csv('./garments_worker_productivity.csv')\n",
    "\n",
    "#Drops the column date\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "#prints all the unique items\n",
    "print('Unique days: ', df.day.unique(), '\\nUnique quarter attributes: ', df.quarter.unique(),\n",
    "      '\\nUnique department attributes: ', df.department.unique(), '\\nUnique team attributes: ', df.team.unique())\n",
    "\n",
    "#fixes the one typo found from the unique elements listed above\n",
    "df.loc[:, \"department\"] = df.department.replace(\"finishing \", \"finishing\")\n",
    "print('\\nUnique department attributes (after fixing typos):',df.department.unique())\n",
    "\n",
    "#adds satisfied column using if actual_productivity is equal to or greater than targeted_productivity\n",
    "df['satisfied'] = (df['actual_productivity'] >= df['targeted_productivity'])\n",
    "\n",
    "#drops the actual_productivity and targeted_productivity columns\n",
    "df = df.drop(columns=['actual_productivity'])\n",
    "df = df.drop(columns=['targeted_productivity'])\n",
    "\n",
    "#finds columns with empty values\n",
    "emptyValueColumns = df.columns[df.isna().any()].tolist()\n",
    "print('\\nColumns/Attributes with empty values:', emptyValueColumns)\n",
    "\n",
    "#fills the empty values with 0's\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Naïve Bayes Classifier (40 points in total)\n",
    "\n",
    "### Exercise 2.1 - Additional Data Preprocessing (10 points)\n",
    "\n",
    "To build a Naïve Bayes Classifier, we need to further encode our categorical variables.\n",
    "\n",
    " - For each of the **categorical attribtues**, encode the set of categories to be **0 ~ (n_classes - 1)**.\n",
    "     - For example, \\[\"paris\", \"paris\", \"tokyo\", \"amsterdam\"\\] should be encoded as \\[1, 1, 2, 0\\].\n",
    "     - Note that the order does not really matter, i.e., \\[0, 0, 1, 2\\] also works. But you have to start with 0 in your encodings.\n",
    "     - You can find information about this encoding in the discussion materials.\n",
    "\n",
    "\n",
    " - Split the data into training and testing set with the ratio of 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after being Label Encoded: \n",
      "\n",
      "       quarter  department  day  team    smv     wip  over_time  incentive  \\\n",
      "0           0           1    3     7  26.16  1108.0       7080         98   \n",
      "1           0           0    3     0   3.94     0.0        960          0   \n",
      "2           0           1    3    10  11.41   968.0       3660         50   \n",
      "3           0           1    3    11  11.41   968.0       3660         50   \n",
      "4           0           1    3     5  25.90  1170.0       1920         50   \n",
      "...       ...         ...  ...   ...    ...     ...        ...        ...   \n",
      "1192        1           0    5     9   2.90     0.0        960          0   \n",
      "1193        1           0    5     7   3.90     0.0        960          0   \n",
      "1194        1           0    5     6   3.90     0.0        960          0   \n",
      "1195        1           0    5     8   2.90     0.0       1800          0   \n",
      "1196        1           0    5     5   2.90     0.0        720          0   \n",
      "\n",
      "      idle_time  idle_men  no_of_style_change  no_of_workers  satisfied  \n",
      "0           0.0         0                   0           59.0       True  \n",
      "1           0.0         0                   0            8.0       True  \n",
      "2           0.0         0                   0           30.5       True  \n",
      "3           0.0         0                   0           30.5       True  \n",
      "4           0.0         0                   0           56.0       True  \n",
      "...         ...       ...                 ...            ...        ...  \n",
      "1192        0.0         0                   0            8.0      False  \n",
      "1193        0.0         0                   0            8.0      False  \n",
      "1194        0.0         0                   0            8.0      False  \n",
      "1195        0.0         0                   0           15.0      False  \n",
      "1196        0.0         0                   0            6.0      False  \n",
      "\n",
      "[1197 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#Note: The programs must be run in order to work\n",
    "\n",
    "# Remember to continue the task with your processed data from Exercise 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Using Label Encoding Code from 10/27 Discussion\n",
    "df.loc[:,'day'] = LabelEncoder().fit_transform(df['day'])\n",
    "df.loc[:,'quarter'] = LabelEncoder().fit_transform(df['quarter'])\n",
    "df.loc[:,'department'] = LabelEncoder().fit_transform(df['department'])\n",
    "df.loc[:,'team'] = LabelEncoder().fit_transform(df['team'])\n",
    "\n",
    "print('Results after being Label Encoded: \\n\\n', df)\n",
    "\n",
    "#Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['satisfied']), df['satisfied'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - Naïve Bayes Classifier for Categorical Attributes (15 points)\n",
    "\n",
    "Use the categorical attributes **only**, please build a Categorical Naïve Bayes classifier that predicts the column `satisfied`. <br >\n",
    "Report the **testing result** using `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.19      0.29        70\n",
      "        True       0.74      0.96      0.84       170\n",
      "\n",
      "    accuracy                           0.73       240\n",
      "   macro avg       0.70      0.57      0.56       240\n",
      "weighted avg       0.71      0.73      0.68       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note: The programs must be run in order to work\n",
    "#Note: Exercise 2.1 must be run before this code can work\n",
    "\n",
    "# Remember to do this task with your processed data from Exercise 2.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#set the data frame for training and testing\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "#fill the data frame columns with the training and testing data\n",
    "train_df['day'] = X_train['day']\n",
    "test_df['day'] = X_test['day']\n",
    "\n",
    "train_df['quarter'] = X_train['quarter']\n",
    "test_df['quarter'] = X_test['quarter']\n",
    "\n",
    "train_df['department'] = X_train['department']\n",
    "test_df['department'] = X_test['department']\n",
    "\n",
    "train_df['team'] = X_train['team']\n",
    "test_df['team'] = X_test['team']\n",
    "\n",
    "#from 10/27 Discussion Code: CategoricalNB\n",
    "NB = CategoricalNB()\n",
    "NB.fit(train_df, y_train)\n",
    "\n",
    "#Reporting the testing results using classification_report\n",
    "#print('Training results: \\n', classification_report(y_train, NB.predict(train_df)))\n",
    "print('Testing results: \\n', classification_report(y_test, NB.predict(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 - Naïve Bayes Classifier for Numerical Attributes (15 points)\n",
    "\n",
    "Use the numerical attributes **only**, please build a Gaussian Naïve Bayes classifier that predicts the column `satisfied`. <br >\n",
    "Report the **testing result** using `classification_report`.\n",
    "\n",
    "**Remember to scale your data. The scaling method is up to you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.04      0.08        70\n",
      "        True       0.72      0.99      0.83       170\n",
      "\n",
      "    accuracy                           0.72       240\n",
      "   macro avg       0.73      0.52      0.46       240\n",
      "weighted avg       0.73      0.72      0.61       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note: The programs must be run in order to work\n",
    "#Note: Exercise 2.1 must be run before this code can work\n",
    "\n",
    "# Remember to do this task with your processed data from Exercise 2.1\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Remove the non numerical columns\n",
    "X_train.drop(columns=['quarter', 'day', 'department', 'team'])\n",
    "X_test.drop(columns=['quarter', 'day', 'department', 'team'])\n",
    "\n",
    "#from 10/27 Discussion Code: Gaussian Naïve Bayes\n",
    "scaler = StandardScaler()\n",
    "NB = GaussianNB()\n",
    "scaler.fit(X_train)\n",
    "NB.fit(scaler.transform(X_train), np.asarray(y_train))\n",
    "\n",
    "#Reporting the testing results using classification_report\n",
    "#print('Training results: \\n', classification_report(y_train, NB.predict(scaler.transform(X_train))))\n",
    "print('Testing results: \\n', classification_report(y_test, NB.predict(scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercies 3 - SVM Classifier (40 points in total)\n",
    "\n",
    "### Exercise 3.1 - Additional Data Preprocessing (10 points)\n",
    "\n",
    "To build a SVM Classifier, we need a different encoding for our categorical variables.\n",
    "\n",
    " - For each of the **categorical attribtues**, encode them with **one-hot encoding**.\n",
    "     - You can find information about this encoding in the discussion materials.\n",
    "\n",
    "\n",
    " - Split the data into training and testing set with the ratio of 80:20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after being One-Hot Encoded: \n",
      "\n",
      "         smv     wip  over_time  incentive  idle_time  idle_men  \\\n",
      "0     26.16  1108.0       7080         98        0.0         0   \n",
      "1      3.94     0.0        960          0        0.0         0   \n",
      "2     11.41   968.0       3660         50        0.0         0   \n",
      "3     11.41   968.0       3660         50        0.0         0   \n",
      "4     25.90  1170.0       1920         50        0.0         0   \n",
      "...     ...     ...        ...        ...        ...       ...   \n",
      "1192   2.90     0.0        960          0        0.0         0   \n",
      "1193   3.90     0.0        960          0        0.0         0   \n",
      "1194   3.90     0.0        960          0        0.0         0   \n",
      "1195   2.90     0.0       1800          0        0.0         0   \n",
      "1196   2.90     0.0        720          0        0.0         0   \n",
      "\n",
      "      no_of_style_change  no_of_workers  satisfied  quarter_0  ...  team_2  \\\n",
      "0                      0           59.0       True          1  ...       0   \n",
      "1                      0            8.0       True          1  ...       0   \n",
      "2                      0           30.5       True          1  ...       0   \n",
      "3                      0           30.5       True          1  ...       0   \n",
      "4                      0           56.0       True          1  ...       0   \n",
      "...                  ...            ...        ...        ...  ...     ...   \n",
      "1192                   0            8.0      False          0  ...       0   \n",
      "1193                   0            8.0      False          0  ...       0   \n",
      "1194                   0            8.0      False          0  ...       0   \n",
      "1195                   0           15.0      False          0  ...       0   \n",
      "1196                   0            6.0      False          0  ...       0   \n",
      "\n",
      "      team_3  team_4  team_5  team_6  team_7  team_8  team_9  team_10  team_11  \n",
      "0          0       0       0       0       1       0       0        0        0  \n",
      "1          0       0       0       0       0       0       0        0        0  \n",
      "2          0       0       0       0       0       0       0        1        0  \n",
      "3          0       0       0       0       0       0       0        0        1  \n",
      "4          0       0       1       0       0       0       0        0        0  \n",
      "...      ...     ...     ...     ...     ...     ...     ...      ...      ...  \n",
      "1192       0       0       0       0       0       0       1        0        0  \n",
      "1193       0       0       0       0       1       0       0        0        0  \n",
      "1194       0       0       0       1       0       0       0        0        0  \n",
      "1195       0       0       0       0       0       1       0        0        0  \n",
      "1196       0       0       1       0       0       0       0        0        0  \n",
      "\n",
      "[1197 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remember to continue the task with your processed data from Exercise 1\n",
    "\n",
    "#one-hot encoding using get_dummies (Piazza #249)\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "df = pd.get_dummies(df, columns=['quarter', 'day', 'department', 'team'])\n",
    "\n",
    "#Split the data into training and testing (Same as 2.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['satisfied']), df['satisfied'], test_size=0.2)\n",
    "\n",
    "print('Results after being One-Hot Encoded: \\n\\n',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 - SVM with Different Kernels (20 points)\n",
    "\n",
    "Using all the attributes we have, please build a SVM that predicts the column `satisfied`. <br >\n",
    "Specifically, please \n",
    " - Build one SVM with **linear kernel**.\n",
    " - Build another SVM but with **rbf kernel**.\n",
    " - Report the **testing results** of **both models** using `classification report`.\n",
    "\n",
    "The kernel is the only setting requirement. <br >\n",
    "Other hyperparameter tuning is not required. But make sure they are the same in these two SVMs if you'd like to tune the model. In other words, the only difference between the two SVMs should be the kernel setting.\n",
    "\n",
    "**Remember to scale your data. The scaling method is up to you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.60      0.27      0.37        66\n",
      "        True       0.77      0.93      0.84       174\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.69      0.60      0.61       240\n",
      "weighted avg       0.72      0.75      0.71       240\n",
      " \n",
      "\n",
      "Linear Kernel Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.29      0.39        66\n",
      "        True       0.78      0.93      0.85       174\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.69      0.61      0.62       240\n",
      "weighted avg       0.73      0.75      0.72       240\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remember to do this task with your processed data from Exercise 3.1\n",
    "\n",
    "#Adapted from 11/03 Discussion Code\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf_li = LinearSVC(dual=False)\n",
    "\n",
    "#scaler from Exercise 2.3\n",
    "scaler.fit(X_train)\n",
    "\n",
    "clf.fit(scaler.transform(X_train), np.asarray(y_train))\n",
    "clf_li.fit(scaler.transform(X_train), np.asarray(y_train))\n",
    "\n",
    "print('RBF Kernel Testing results: \\n',classification_report(y_test, clf.predict(scaler.transform(X_test))), '\\n')\n",
    "print('Linear Kernel Testing results: \\n',classification_report(y_test, clf_li.predict(scaler.transform(X_test))), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 - SVM with Over-sampling (10 points)\n",
    " - For the column `satisfied` in our **training set**, please **print out** the frequency of each class. \n",
    " - Oversample the **training data**. \n",
    " - For the column `satisfied` in the oversampled data, **print out** the frequency of each class  again.\n",
    " - Re-build the 2 SVMs with the same setting you have in Exercise 3.2, but **use oversampled training data** instead.\n",
    "     - Do not forget to scale the data first. As always, the scaling method is up to you.\n",
    " - Report the **testing result** with `classification_report`.\n",
    "\n",
    "You can use ANY methods listed on [here](https://imbalanced-learn.org/stable/references/over_sampling.html#) such as RandomOverSampler or SMOTE. <br > \n",
    "You are definitely welcomed to build your own oversampler. <br >\n",
    "\n",
    "Note that you do not have to over-sample your testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency before oversampling:\n",
      "\n",
      "True     701\n",
      "False    256\n",
      "Name: satisfied, dtype: int64 \n",
      "\n",
      "Frequency after oversampling:\n",
      "\n",
      "False    701\n",
      "True     701\n",
      "Name: satisfied, dtype: int64 \n",
      "\n",
      "RBF Kernel Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.35      0.44        66\n",
      "        True       0.79      0.91      0.84       174\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.69      0.63      0.64       240\n",
      "weighted avg       0.73      0.75      0.73       240\n",
      " \n",
      "\n",
      "Linear Kernel Testing results: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.30      0.40        66\n",
      "        True       0.78      0.92      0.84       174\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.68      0.61      0.62       240\n",
      "weighted avg       0.72      0.75      0.72       240\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remember to do this task with your processed data from Exercise 3.1\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#finding the frequency before oversampling\n",
    "frequeny_preOverSampling = y_train.value_counts()\n",
    "print(\"Frequency before oversampling:\\n\")\n",
    "print(frequeny_preOverSampling,'\\n')\n",
    "\n",
    "#https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "smote = SMOTE(random_state=12)\n",
    "x_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#same as Exercise 3.2\n",
    "scaler.fit(x_train_oversampled)\n",
    "clf.fit(scaler.transform(x_train_oversampled), np.asarray(y_train_oversampled))\n",
    "clf_li.fit(scaler.transform(x_train_oversampled), np.asarray(y_train_oversampled))\n",
    "\n",
    "#finding the frequency before oversampling\n",
    "frequeny_postOverSampling = y_train_oversampled.value_counts()\n",
    "print(\"Frequency after oversampling:\\n\")\n",
    "print(frequeny_postOverSampling,'\\n')\n",
    "\n",
    "print('RBF Kernel Testing results: \\n',classification_report(y_test, clf.predict(scaler.transform(X_test))), '\\n')\n",
    "print('Linear Kernel Testing results: \\n',classification_report(y_test, clf_li.predict(scaler.transform(X_test))), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
